import random


# Генерация словаря с набором параметров для SNN
def generate_params():
    """

    Зависимости между параметрами:

    1) t_ltp - верхняя граница значения разности между временем поступления
       входного сигнала и временем спайка нейрона, при котором мы можем считать,
       что входной сигнал пришел "недавно";
       t_ref - время, на которое "выключается" нейрон после спайка;
       t_inhibit - время, на которое сработавший нейрон "глушит" остальные;
       tau_leak - постоянная времени утечки потенциала;
                  определяет, насколько долго нейрон помнит предыдущие входы;
                  если tau_leak слишком маленькая, то входной потенциал не накопится до порога;
                  если слишком большая, то сигнал от старых входов будет влиять слишком долго.

    2) Константы времени задаются в мс исходя из того, что при текущих настройках
       потока событий на обраюотку одной пары (направление, картинка) затрачивается 50 мс;
       За это время не менее 6 нейронов (если в сети 24 нейрона и 4 возможных направления)
       должны выдать несколько спайков.

    3) t_inhibit < t_ref, чтобы дать возможность нескольким нейронам среагировать на
       одну и ту же пару (направление, картинка).

    4) t_inhibit < t_ltp и t_ref < t_ltp, чтобы нейроны успевали отреагировать на входы.
       
    5) tau_leak должно быть немного больше, чем t_ltp, чтобы входы, поступившие во время
       обучения на одной паре (направление, картинка), не забывались.

    6) Начальные веса связей (w_init) задаются нормальным распределением 
       со средним w_init_mean и дисперсией w_init_std.

    7) Пороговое значение потенциала нейрона для генерации спайка I_thres
       должно быть таким, чтобы при выбранных значениях STDP нейрон успевал
       спайковать ожидаемое количество раз.

    Значения всех констант времени в мс.

    """
    # Окно отклика нейрона должно быть таким, чтобы за время обработки 
    # одной картинки "сильный" нейрон успевал выдать несколько спайков
    t_ltp = random.uniform(5.0, 15.0)

    # Период ингибирования должен быть меньше, чем длительность рефрактерного 
    # периода и меньше, чем t_ltp, поэтому берем малый диапазон
    t_inhibit = random.uniform(1.0, t_ltp - 2.0)

    # t_inhibit < t_ref < t_ltp
    t_ref_min = t_inhibit + 0.5
    t_ref_max = max(t_ref_min + 0.5, t_ltp - 0.5)
    if t_ref_max < t_ref_min:
        t_ref_max = t_ref_min + 0.5
    t_ref = random.uniform(t_ref_min, t_ref_max)

    # tau_leak должно быть немного больше, чем t_ltp
    tau_leak_min = t_ltp + 1.0
    tau_leak_max = t_ltp + 10.0
    tau_leak = random.uniform(tau_leak_min, tau_leak_max)

    # Границы I_thres и коэффициентов STDP установлены после 
    # перебора различных комбинаций значений и построения графиков
    # (веса, растр спайков, предпочтения нейронов)
    I_thres = random.uniform(1000.0, 2800.0)
    alpha_plus = random.uniform(20.0, 60.0)
    alpha_minus = random.uniform(5.0, 20.0)
    beta_plus = random.uniform(0.01, 0.1)
    beta_minus = random.uniform(0.05, 0.3)
    w_init_mean = random.uniform(200.0, 320.0)
    w_init_std  = random.uniform(20.0, 60.0)
    w_min = 20.0
    w_max = 600.0

    return{
        "I_thres": I_thres,
        "t_ltp": t_ltp,
        "t_ref": t_ref,
        "t_inhibit": t_inhibit,
        "tau_leak": tau_leak,
        "alpha_plus": alpha_plus,
        "alpha_minus": alpha_minus,
        "beta_plus": beta_plus,
        "beta_minus": beta_minus,
        "w_init_mean": w_init_mean,
        "w_init_std": w_init_std,
        "w_min": w_min,
        "w_max": w_max
    }



# Получение словаря параметров потомка из словарей родителей
def mix_params(p1, p2):
    """

    p1, p2: словари с параметрами STDP, LIF, SNN

    """
    # Инициализируем новый словарь
    child = {}

    # Идем по ключам (наименования параметров)
    for key in p1.keys():
        # Извлекаем значения параметров родителей
        param_p1 = p1[key]
        param_p2 = p2[key]

        # Присваиваем потомку значение по ключу 
        # (равновероятный выбор)
        child[key] = (param_p1 if random.random() < 0.5 else param_p2)

    return child



# Получение нового словаря из старого путем мутаций
def mutate_params(
        parent,             # исходный словарь параметров
        mutation_prob=0.2,  # вероятность мутации
):
    # Создаем копию исходного словаря
    child = dict(parent)

    # Идем по ключам (наименования параметров)
    for key in parent.keys():
        # Предельные значения весов зафиксированы, их пропускаем
        if key in ["w_min", "w_max"]:
            continue
        
        # Решаем, нужна ли мутация
        if random.random() < mutation_prob:
            # Подбираем коэффициент изменения значения параметра
            rate = random.uniform(0.8, 1.2)
            child[key] = parent[key] * rate

    return child


# Отбор лучших кандидатов поколения
def candidate_selection(
        candidates,         # список кандидатов в формате (точность, словарь параметров)
        group_size=2        # сколько случайных кандидатов сравниваем
):
    # Выбираем случайную группу из всех кандидатов
    group = random.sample(candidates, group_size)
    # Сортируем подгруппу по точности (от лучшего к худшему)
    group.sort(key=lambda x: x[0], reverse=True)
    # Возвращаем самого точного кандидата
    return group[0] 
